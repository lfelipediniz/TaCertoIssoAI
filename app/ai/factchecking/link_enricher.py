"""
Link Enrichment Module - Step 2.5 of Fact-Checking Pipeline

This module takes claims with URLs and enriches them by extracting content from those URLs.
Based on the existing webscraping module using newspaper3k library.

Follows LangChain best practices:
- Async-first design
- Structured outputs with Pydantic models
- Error handling and fallback mechanisms
- Detailed logging and processing notes
"""

import asyncio
import time
import logging
import re
import random
import requests
from bs4 import BeautifulSoup

from newspaper import Article, Config
from readability import Document
import trafilatura
from goose3 import Goose

from app.models.factchecking import (
    ClaimExtractionResult,
    ExtractedClaim,
    EnrichedLink,
    EnrichedClaim,
    LinkEnrichmentResult
)

logger = logging.getLogger(__name__)


def _is_invalid_content(texto):
    """
    Verifica se o conte√∫do extra√≠do √© inv√°lido (ex: mensagens de JavaScript desabilitado)
    """
    if not texto:
        return True
    
    texto_lower = texto.lower().strip()
    
    # Lista expandida de padr√µes que indicam conte√∫do inv√°lido
    invalid_patterns = [
        # JavaScript/erro patterns
        "javascript is not available",
        "javascript is disabled",
        "please enable javascript",
        "switch to a supported browser",
        "we've detected that javascript is disabled",
        "enable javascript or switch to a supported browser",
        "something went wrong",
        "try again",
        "privacy related extensions",
        "disable them and try again",
        
        # X/Twitter specific patterns
        "help center",
        "terms of service",
        "privacy policy",
        "cookie policy",
        "ads info",
        "imprint",
        "¬© 2025 x corp",
        "¬© 2024 x corp",
        "¬© 2023 x corp",
        
        # Generic error patterns
        "access denied",
        "forbidden",
        "not found",
        "page not found",
        "server error",
        "temporarily unavailable",
        "maintenance mode",
        "under construction",
        "coming soon",
        "this page is not available",
        "content not available",
        "unable to load",
        "loading failed",
        "connection error",
        "timeout",
        "rate limited",
        "too many requests"
    ]
    
    # Verifica se o texto cont√©m principalmente padr√µes inv√°lidos
    invalid_count = sum(1 for pattern in invalid_patterns if pattern in texto_lower)
    total_words = len(texto_lower.split())
    
    # Se mais de 15% das palavras s√£o padr√µes inv√°lidos, considera inv√°lido
    if total_words > 0 and (invalid_count / total_words) > 0.15:
        return True
    
    # Se o texto cont√©m qualquer padr√£o inv√°lido e √© curto
    if len(texto_lower) < 300 and any(pattern in texto_lower for pattern in invalid_patterns):
        return True
    
    # Verifica se o texto √© principalmente sobre JavaScript/erro
    js_related_words = ["javascript", "browser", "enable", "switch", "supported", "detected", "error", "failed", "unavailable"]
    js_word_count = sum(1 for word in js_related_words if word in texto_lower)
    
    if js_word_count >= 2 and len(texto_lower) < 400:
        return True
    
    # Verifica se o texto √© muito curto e cont√©m palavras de erro
    error_words = ["error", "failed", "unavailable", "denied", "forbidden", "not found", "disabled"]
    if len(texto_lower) < 100 and any(word in texto_lower for word in error_words):
        return True
    
    # Verifica se o texto √© principalmente links de navega√ß√£o/footer
    footer_words = ["help", "terms", "privacy", "policy", "cookie", "ads", "imprint", "corp", "¬©"]
    footer_count = sum(1 for word in footer_words if word in texto_lower)
    
    if footer_count >= 3 and len(texto_lower) < 200:
        return True
    
    return False


def _extrair_com_trafilatura(url):
    """M√©todo 1: Trafilatura (muito robusto)"""
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            content = trafilatura.extract(downloaded, include_comments=False, include_tables=True)
            metadata = trafilatura.extract_metadata(downloaded)
            
            return {
                "titulo": metadata.title if metadata else None,
                "autores": [metadata.author] if metadata and metadata.author else None,
                "data_publicacao": metadata.date if metadata else None,
                "texto_completo": content
            }
    except Exception as e:
        logger.debug(f"Trafilatura extraction failed for {url}: {e}")
    return None


def _extrair_com_newspaper3k(url):
    """M√©todo 2: Newspaper3k (especializado em not√≠cias)"""
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        config.request_timeout = 10
        
        artigo = Article(url, language='pt', config=config)
        artigo.download()
        artigo.parse()
        
        return {
            "titulo": artigo.title,
            "autores": artigo.authors,
            "data_publicacao": artigo.publish_date,
            "texto_completo": artigo.text
        }
    except Exception as e:
        logger.debug(f"Newspaper3k extraction failed for {url}: {e}")
    return None


def _extrair_com_readability(url):
    """M√©todo 3: Readability-lxml (focado em conte√∫do principal)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            doc = Document(response.text)
            soup = BeautifulSoup(doc.summary(), 'html.parser')
            
            # Extrair t√≠tulo da p√°gina original
            title_soup = BeautifulSoup(response.text, 'html.parser')
            title = title_soup.find('title')
            title_text = title.get_text().strip() if title else None
            
            return {
                "titulo": title_text,
                "autores": None,
                "data_publicacao": None,
                "texto_completo": soup.get_text()
            }
    except Exception as e:
        logger.debug(f"Readability extraction failed for {url}: {e}")
    return None


def _extrair_com_goose3(url):
    """M√©todo 4: Goose3 (especializado em not√≠cias)"""
    try:
        g = Goose()
        article = g.extract(url=url)
        
        return {
            "titulo": article.title,
            "autores": [article.authors] if article.authors else None,
            "data_publicacao": article.publish_date,
            "texto_completo": article.cleaned_text
        }
    except Exception as e:
        logger.debug(f"Goose3 extraction failed for {url}: {e}")
    return None


def _extrair_com_requests_session(url):
    """M√©todo 5: Requests com sess√£o (para sites que requerem cookies)"""
    try:
        session = requests.Session()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        session.headers.update(headers)
        
        # Primeira requisi√ß√£o para estabelecer sess√£o
        response = session.get(url, timeout=15, allow_redirects=True)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Remove scripts e estilos
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()
            
            # Tenta encontrar t√≠tulo
            title = soup.find('title')
            title_text = title.get_text().strip() if title else None
            
            # Para X/Twitter, tenta seletores espec√≠ficos
            if 'x.com' in url or 'twitter.com' in url:
                content_selectors = [
                    '[data-testid="tweetText"]',  # Texto do tweet
                    '[data-testid="tweet"]',      # Container do tweet
                    'article[data-testid="tweet"]',  # Artigo do tweet
                    '[role="article"]',           # Artigo gen√©rico
                    '.tweet-text',                # Classe do texto do tweet
                    '[data-testid="card.wrapper"]'  # Card wrapper
                ]
            else:
                content_selectors = [
                    'article', 'main', '.content', '.post-content', 
                    '.article-content', '.entry-content', '[role="main"]'
                ]
            
            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text()
                    if content and len(content.strip()) > 10:
                        break
            
            if not content:
                content = soup.get_text()
            
            # Limpa o texto
            content = re.sub(r'\s+', ' ', content).strip()
            
            return {
                "titulo": title_text,
                "autores": None,
                "data_publicacao": None,
                "texto_completo": content
            }
    except Exception as e:
        logger.debug(f"Requests session extraction failed for {url}: {e}")
    return None


def _extrair_com_beautifulsoup(url):
    """M√©todo 6: BeautifulSoup (√∫ltimo recurso)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Remove scripts e estilos
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()
            
            # Tenta encontrar t√≠tulo
            title = soup.find('title')
            title_text = title.get_text().strip() if title else None
            
            # Tenta encontrar conte√∫do principal
            content_selectors = [
                'article', 'main', '.content', '.post-content', 
                '.article-content', '.entry-content', '[role="main"]'
            ]
            
            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text()
                    break
            
            if not content:
                content = soup.get_text()
            
            # Limpa o texto
            content = re.sub(r'\s+', ' ', content).strip()
            
            return {
                "titulo": title_text,
                "autores": None,
                "data_publicacao": None,
                "texto_completo": content
            }
    except Exception as e:
        logger.debug(f"BeautifulSoup extraction failed for {url}: {e}")
    return None


def extrair_noticia_principal_de_link(url):
    """
    Tenta extrair o conte√∫do de uma not√≠cia usando m√∫ltiplas bibliotecas em sequ√™ncia.
    Usa uma abordagem de fallback otimizada: m√©todos r√°pidos primeiro, pesados depois.
    
    Args:
        url (str): O link da not√≠cia.
        
    Returns:
        dict: Um dicion√°rio com o t√≠tulo, autores, data e o texto limpo da not√≠cia.
        None: Se todas as tentativas falharem.
    """
    
    # PIPELINE OTIMIZADO: M√©todos r√°pidos primeiro, pesados depois
    # Fase 1: M√©todos r√°pidos e baratos (HTTP requests)
    metodos_rapidos = [
        ("trafilatura", _extrair_com_trafilatura),
        ("newspaper3k", _extrair_com_newspaper3k),
        ("readability", _extrair_com_readability),
        ("requests_session", _extrair_com_requests_session),
        ("beautifulsoup", _extrair_com_beautifulsoup)
    ]
    
    # Para X/Twitter, adiciona goose3 na fase r√°pida
    if 'x.com' in url or 'twitter.com' in url:
        metodos_rapidos.insert(1, ("goose3", _extrair_com_goose3))
    
    logger.debug(f"üöÄ FASE 1: Tentando m√©todos r√°pidos para {url}")
    
    # Tenta m√©todos r√°pidos primeiro
    for nome_metodo, funcao_metodo in metodos_rapidos:
        try:
            logger.debug(f"Tentando extrair com {nome_metodo}...")
            resultado = funcao_metodo(url)
            
            if resultado and resultado.get('texto_completo'):
                texto = resultado['texto_completo'].strip()
                
                # Verifica se o conte√∫do √© v√°lido
                if len(texto) > 50 and not _is_invalid_content(texto):
                    logger.debug(f"‚úÖ Sucesso com {nome_metodo}!")
                    resultado['metodo_usado'] = nome_metodo
                    return resultado
                else:
                    logger.debug(f"‚ùå {nome_metodo} extraiu conte√∫do inv√°lido (JS disabled ou muito curto)")
            else:
                logger.debug(f"‚ùå {nome_metodo} n√£o conseguiu extrair conte√∫do suficiente")
                
        except Exception as e:
            logger.debug(f"‚ùå Erro com {nome_metodo}: {e}")
            continue
    
    logger.debug(f"‚ùå Todos os m√©todos falharam para {url}")
    return None


class LinkEnricher:
    """
    Enriches claims by extracting content from their associated URLs.
    
    Uses newspaper3k for web scraping with async execution.
    """
    
    def __init__(self, content_limit: int = 5000):
        """Initialize the link enricher with web scraping capabilities."""
        
        # Configure newspaper3k for web scraping
        self.newspaper_config = Config()
        self.newspaper_config.browser_user_agent = (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
        self.newspaper_config.request_timeout = 15  # Increased timeout
        
        # Content limit for extracted text
        self.content_limit = content_limit

    async def enrich_links(self, claims_result: ClaimExtractionResult) -> LinkEnrichmentResult:
        """
        Main method to enrich all claims with link content.
        
        Args:
            claims_result: Result from claim extraction step
            
        Returns:
            LinkEnrichmentResult with enriched claims
        """
        start_time = time.time()
        
        enriched_claims = []
        total_links = 0
        successful_extractions = 0
        
        for claim in claims_result.claims:
            if claim.links:
                total_links += len(claim.links)
                enriched_claim = await self._enrich_single_claim(claim)
                
                # Count successful extractions
                for enriched_link in enriched_claim.enriched_links:
                    if enriched_link.extraction_status == "success":
                        successful_extractions += 1
                        
                enriched_claims.append(enriched_claim)
            else:
                # No links to enrich, convert to EnrichedClaim as-is
                enriched_claim = EnrichedClaim(
                    text=claim.text,
                    original_links=[],
                    enriched_links=[],
                    llm_comment=claim.llm_comment,
                    entities=claim.entities
                )
                enriched_claims.append(enriched_claim)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        processing_notes = (
            f"Processados {total_links} links. "
            f"{successful_extractions} extra√ß√µes bem-sucedidas. "
            f"{total_links - successful_extractions} falhas."
        )
        
        return LinkEnrichmentResult(
            original_claims=claims_result.claims,
            enriched_claims=enriched_claims,
            total_links_processed=total_links,
            successful_extractions=successful_extractions,
            processing_time_ms=processing_time,
            processing_notes=processing_notes
        )

    async def _enrich_single_claim(self, claim: ExtractedClaim) -> EnrichedClaim:
        """Enrich a single claim by extracting content from its links."""
        
        enriched_links = []
        
        # Process each link in the claim
        for url in claim.links:
            enriched_link = await self._extract_link_content(url)
            enriched_links.append(enriched_link)
        
        return EnrichedClaim(
            text=claim.text,
            original_links=claim.links,
            enriched_links=enriched_links,
            llm_comment=claim.llm_comment,
            entities=claim.entities
        )

    async def _extract_link_content(self, url: str) -> EnrichedLink:
        """
        Extract content from a single URL using newspaper3k.
        
        Args:
            url: The URL to extract content from
            
        Returns:
            EnrichedLink with extracted content and summary
        """
        enriched_link = EnrichedLink(
            url=url,
            extraction_status="pending"
        )
        
        try:
            # Use asyncio to run newspaper3k extraction in thread pool
            # (newspaper3k is synchronous, so we need to wrap it)
            loop = asyncio.get_event_loop()
            extraction_result = await loop.run_in_executor(
                None, 
                self._extract_with_newspaper, 
                url
            )
            
            if extraction_result:
                enriched_link.title = extraction_result.get("titulo", "")
                
                # Apply content limit from the existing webscraping logic
                full_content = extraction_result.get("texto_completo", "")
                enriched_link.content = full_content[:self.content_limit] if full_content else ""
                
                # Create a simple summary based on title and first paragraph
                enriched_link.summary = self._create_simple_summary(
                    enriched_link.title, 
                    enriched_link.content
                )
                
                enriched_link.extraction_status = "success"
                method_used = extraction_result.get("metodo_usado", "unknown")
                enriched_link.extraction_notes = f"Conte√∫do extra√≠do com {method_used}. Tamanho: {len(full_content)} chars, limitado a {len(enriched_link.content)} chars"
            else:
                enriched_link.extraction_status = "failed"
                enriched_link.extraction_notes = "Falha na extra√ß√£o de conte√∫do"
                
        except Exception as e:
            logger.error(f"Link enrichment failed for {url}: {e}")
            enriched_link.extraction_status = "failed"
            enriched_link.extraction_notes = f"Erro durante extra√ß√£o: {str(e)[:100]}"
        
        return enriched_link

    def _extract_with_newspaper(self, url: str) -> dict:
        """
        Enhanced extraction using the optimized pipeline with multiple fallback methods.
        Uses the new pipeline that tries multiple extraction methods in order of efficiency.
        """
        try:
            # Use the optimized pipeline with multiple extraction methods
            extraction_result = extrair_noticia_principal_de_link(url)
            
            if extraction_result:
                # Convert to the expected format
                info_noticia = {
                    "titulo": extraction_result.get("titulo", ""),
                    "autores": extraction_result.get("autores", []),
                    "data_publicacao": extraction_result.get("data_publicacao"),
                    "texto_completo": extraction_result.get("texto_completo", ""),
                    "metodo_usado": extraction_result.get("metodo_usado", "unknown")
                }
                
                logger.debug(f"Successful extraction for {url} using {info_noticia['metodo_usado']}")
                return info_noticia
            else:
                logger.warning(f"All extraction methods failed for {url}")
                return None
            
        except Exception as e:
            logger.error(f"Pipeline extraction failed for {url}: {e}")
            return None

    def _create_simple_summary(self, title: str, content: str) -> str:
        """
        Create a simple summary without LLM, using basic text processing.
        """
        if not content:
            return "Conte√∫do n√£o dispon√≠vel"
        
        # Use title and first paragraph as summary
        summary_parts = []
        
        if title:
            summary_parts.append(f"T√≠tulo: {title}")
        
        # Get first paragraph (up to first double newline or first 200 chars)
        first_paragraph = content.split('\n\n')[0] if '\n\n' in content else content
        first_paragraph = first_paragraph[:200] + "..." if len(first_paragraph) > 200 else first_paragraph
        
        if first_paragraph.strip():
            summary_parts.append(f"Resumo: {first_paragraph.strip()}")
        
        return " | ".join(summary_parts) if summary_parts else "Conte√∫do extra√≠do mas sem informa√ß√µes resum√≠veis"


# Factory function for creating enricher
def create_link_enricher(content_limit: int = 5000) -> LinkEnricher:
    """
    Factory function to create a LinkEnricher instance.
    """
    return LinkEnricher(content_limit=content_limit)


# Async helper function for direct usage
async def enrich_claim_links(claims_result: ClaimExtractionResult) -> LinkEnrichmentResult:
    """
    Convenience function to enrich links from claim extraction result.
    
    Args:
        claims_result: ClaimExtractionResult from step 2
        
    Returns:
        LinkEnrichmentResult with enriched claims
    """
    enricher = create_link_enricher()
    return await enricher.enrich_links(claims_result)
